When you look back at the biggest breakthroughs in AI over the last decade,
Google has been at the forefront of so many of them. Our groundbreaking work
 in foundation models has become the bedrock for the industry and the AI-powered
 products that billions of people use daily. As we continue to responsibly advance
 these technologies, there’s great potential for transformational uses in areas as
 far-reaching as healthcare and human creativity.

Over the past decade of developing AI, we’ve learned that so much is possible as
 you scale up neural networks — in fact, we’ve already seen surprising and delightful
  capabilities emerge from larger sized models. But we’ve learned through our research
  that it’s not as simple as “bigger is better,” and that research creativity is key
   to building great models. More recent advances in how we architect and train models
    have taught us how to unlock multimodality, the importance of having human feedback
     in the loop, and how to build models more efficiently than ever. These are powerful
      building blocks as we continue to advance the state of the art in AI while building
       models that can bring real benefit to people in their daily lives.

Introducing PaLM 2
Building on this work, today we’re introducing PaLM 2, our next generation
language model. PaLM 2 is a state-of-the-art language model with improved
multilingual, reasoning and coding capabilities.

Multilinguality: PaLM 2 is more heavily trained on multilingual text,
spanning more than 100 languages. This has significantly improved its
ability to understand, generate and translate nuanced text — including
idioms, poems and riddles — across a wide variety of languages, a hard
 problem to solve. PaLM 2 also passes advanced language proficiency exams
 at the “mastery” level.
Reasoning: PaLM 2’s wide-ranging dataset includes scientific papers and
web pages that contain mathematical expressions. As a result, it demonstrates
 improved capabilities in logic, common sense reasoning, and mathematics.
Coding: PaLM 2 was pre-trained on a large quantity of publicly available source
 code datasets. This means that it excels at popular programming languages like
  Python and JavaScript, but can also generate specialized code in languages like
  Prolog, Fortran and Verilog.
As we continue to develop PaLM 2, we’ll also be introducing version updates for
 PaLM 2 as it is implemented into products.